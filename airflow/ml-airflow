Let's extend the example by incorporating **MySQL Server** for data storage and **Apache Airflow** for orchestrating the entire Machine Learning pipeline. We'll follow these steps:

### Overview
1. **Data Collection**: Store the California Housing dataset in MySQL.
2. **Data Preparation**: Use Apache Airflow to extract data from MySQL, prepare it, and store processed data back in MySQL.
3. **Model Training**: Train the model using data fetched from MySQL.
4. **Model Deployment**: Deploy the model using Flask.
5. **Orchestration with Airflow**: Automate the entire workflow using Apache Airflow.

### Prerequisites
- A virtual machine with Python, MySQL Server, and Apache Airflow installed.
- Required libraries: `mysql-connector-python`, `pandas`, `scikit-learn`, `flask`, `gunicorn`, `apache-airflow`, and `pickle`.

### Step 1: Setting Up MySQL Server

#### 1.1. Install MySQL Server
```bash
sudo apt update
sudo apt install mysql-server -y
sudo systemctl start mysql
sudo systemctl enable mysql
```

#### 1.2. Secure MySQL Installation
```bash
sudo mysql_secure_installation
```

#### 1.3. Create Database and User
```sql
-- Log in to MySQL
sudo mysql -u root -p

-- Create a new database
CREATE DATABASE housing_db;

-- Create a new user
CREATE USER 'mluser'@'localhost' IDENTIFIED BY 'mlpassword';

-- Grant privileges
GRANT ALL PRIVILEGES ON housing_db.* TO 'mluser'@'localhost';
FLUSH PRIVILEGES;
EXIT;
```

---

### Step 2: Data Collection and Storage in MySQL

#### `data_to_mysql.py`
```python
import pandas as pd
import mysql.connector
from sklearn.datasets import fetch_california_housing

# Fetch the California Housing dataset
data = fetch_california_housing()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Connect to MySQL
connection = mysql.connector.connect(
    host='localhost',
    user='mluser',
    password='mlpassword',
    database='housing_db'
)
cursor = connection.cursor()

# Create table if not exists
cursor.execute('''
    CREATE TABLE IF NOT EXISTS housing_data (
        id INT AUTO_INCREMENT PRIMARY KEY,
        MedInc FLOAT, HouseAge FLOAT, AveRooms FLOAT, AveBedrms FLOAT,
        Population FLOAT, AveOccup FLOAT, Latitude FLOAT, Longitude FLOAT, target FLOAT
    )
''')

# Insert data into MySQL
for _, row in df.iterrows():
    cursor.execute('''
        INSERT INTO housing_data (MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude, target)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
    ''', tuple(row))
connection.commit()

print("Data inserted into MySQL successfully.")

# Close connection
cursor.close()
connection.close()
```

**Run the script**:
```bash
python3 data_to_mysql.py
```

---

### Step 3: Setting Up Apache Airflow

#### 3.1. Install Airflow
```bash
pip3 install apache-airflow
airflow db init
```

#### 3.2. Start Airflow Services
```bash
airflow standalone
```

#### 3.3. Create Airflow DAG for ML Pipeline

Create a file named `housing_ml_pipeline.py` in the `dags` folder.

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import pandas as pd
import mysql.connector
from sklearn.ensemble import RandomForestRegressor
import pickle

# MySQL connection details
MYSQL_CONN = {
    'host': 'localhost',
    'user': 'mluser',
    'password': 'mlpassword',
    'database': 'housing_db'
}

def extract_data(**kwargs):
    connection = mysql.connector.connect(**MYSQL_CONN)
    query = "SELECT * FROM housing_data"
    df = pd.read_sql(query, con=connection)
    df.to_csv('/tmp/raw_data.csv', index=False)
    connection.close()

def prepare_data():
    df = pd.read_csv('/tmp/raw_data.csv')
    X = df.drop(columns=['target', 'id'])
    y = df['target']
    X.to_csv('/tmp/X.csv', index=False)
    y.to_csv('/tmp/y.csv', index=False)

def train_model():
    X = pd.read_csv('/tmp/X.csv')
    y = pd.read_csv('/tmp/y.csv')
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)
    
    with open('/tmp/housing_model.pkl', 'wb') as file:
        pickle.dump(model, file)

default_args = {
    'owner': 'ravi',
    'start_date': datetime(2023, 1, 1),
    'retries': 1
}

dag = DAG(
    'housing_ml_pipeline',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag
)

prepare_task = PythonOperator(
    task_id='prepare_data',
    python_callable=prepare_data,
    dag=dag
)

train_task = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag
)

extract_task >> prepare_task >> train_task
```

#### 3.4. Trigger Airflow DAG
```bash
airflow dags list
airflow dags trigger housing_ml_pipeline
```

---

### Step 4: Model Deployment using Flask

#### `app.py`
```python
from flask import Flask, request, jsonify
import pandas as pd
import pickle

app = Flask(__name__)

# Load the trained model
with open('/tmp/housing_model.pkl', 'rb') as file:
    model = pickle.load(file)

@app.route('/')
def home():
    return "California Housing Price Prediction API"

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    df = pd.DataFrame([data])
    prediction = model.predict(df)
    return jsonify({'predicted_price': prediction[0]})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

#### 4.1. Run Flask App
```bash
python3 app.py
```

---

### Step 5: Accessing the Deployed Model

Use `curl` to send a request:
```bash
curl -X POST http://localhost:5000/predict -H "Content-Type: application/json" \
-d '{"MedInc": 8.3, "HouseAge": 30, "AveRooms": 6.0, "AveBedrms": 1.0, "Population": 500, "AveOccup": 3.5, "Latitude": 37.5, "Longitude": -120.2}'
```

Expected Response:
```json
{"predicted_price": 2.345}
```

---

### Summary
- **MySQL** stores raw housing data.
- **Apache Airflow** orchestrates data extraction, preparation, and model training.
- **Flask** serves the trained model via an API endpoint.

This approach ensures your Machine Learning pipeline is automated, scalable, and deployable.
